---
layout: post
title: Retrieval Augmented Generation
subtitle: Giảm thiểu hiện tượng Hallucinations trong các ứng dụng GenAI
cover-img: /assets/img/2023199_Intro-ml-sys-design/information-system-scaled.webp
thumbnail-img: /assets/img/2023199_Intro-ml-sys-design/thumbb.webp
share-img: /assets/img/2023199_Intro-ml-sys-design/information-system-scaled.webp
tags: [Retrieval augmented generation, rag, hallucinations]
---

Các sản phẩm dựa trên các mô hình ngôn ngữ lớn (large language models LLMs) như OpenAi, ChatGPT có những ứng dụng tuyệt vời, tuy nhiên chúng vẫn còn những hạn chế:
1. Chúng không thể cập nhật thông tin: các LLMs chỉ có những kiến thức đên một thời điểm nhất định lúc dữ liệu huấn luyện được thu thập.
2. Chúng không có các kiến thức cụ thể: LLMs được huấn luyện cho các nhiệm vụ chung.
3. Chúng được xem là "black boxes": không dễ để biết được LLM sử dụng nguồn tài liệu ở đâu và cách chúng đưa ra quyết định dựa trên cơ sở nào.
4. Chúng không hiệu quả và đắt tiền: chỉ một ít công ty có đủ tài chính và nguồn nhân lực để triển khai các models nền tảng.

Bài viết này sẽ phân tích tại sao Retrieval Augmented Generation được ưu tiên sử dụng đối phó với những hạn chế kể trên, tìm hiểu sâu về cách RAG làm việt.

## LLMs không thể cập nhật thông tin, nhưng RAG thì có thể

Dữ liệu để huấn luyện ChatGPT chỉ đến tháng 9 năm 2021. Nếu bạn hỏi ChatGPT một vấn đề xảy ra vào tháng trước, nó sẽ không thể đưa ra câu trả lời chính xác và nó sẽ tưởng tượng ra một câu trả lời hết sức thuyết phục, hiện tượng này còn được gọi là "hallucination".

![alt text](/assets/img/20231008_rag/hallu.png)

LLMs thiết thông tin cụ thể về Volvo XC60 trong ví dụ trên và nó cố gắng trả lời trông có vẻ rất đúng nhưng các thông tin đều không chính xác.

## RAG cung cấp các thông tin cập nhật về thế giới và các dữ liệu mang tính cụ thể cho các ứng dụng GenAI

Retrieval Augmented Generation có nghĩa là lấy các thông tin cập nhật hoặc các dữ liệu từ database và biến chúng trở thành kiến thức của LLM. Chúng ta có thể lưu các dữ liệu về kinh doanh hoặc thông tin về thế giới và để cho LLM "biết" về nó trong lúc nó trả lời câu hỏi, điều này giúp làm giảm hiện tượng hallucinations.

## LLMs thiếu các nội dung từ dữ liệu riêng tư - dẫn đến hallucinations khi được hỏi các câu hỏi có tính cụ thể như về thông tin riêng tư của công ty

Nhược điểm thứ 2 của LLMs là mặc dù các kiến thức nền của chúng là khổng lồ, thì chúng vẫn không có thông tin về các yêu cầu của bạn, khách hàng của bạn, hoặc về cửa hàng của bạn.

RAG giúp giải quyết vấn đề này bằng cách thêm vào các ngữ cảnh và thông tin đến LLMs lúc nó đưa ra câu trả lời: có thể là bản ghi của khách hàng, kịch bản, mô tả sản phẩm, giá chứng khoán,..

